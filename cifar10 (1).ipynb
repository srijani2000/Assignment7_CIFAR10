{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T12:00:24.367544Z","iopub.execute_input":"2025-10-04T12:00:24.367853Z","iopub.status.idle":"2025-10-04T12:00:24.372692Z","shell.execute_reply.started":"2025-10-04T12:00:24.367832Z","shell.execute_reply":"2025-10-04T12:00:24.371879Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# ===============================\n# ðŸ“Œ Block 1: Install & Import Libraries\n# ===============================\n!pip install albumentations==1.4.3\n!pip install torchsummary\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchsummary import summary\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T12:00:24.373920Z","iopub.execute_input":"2025-10-04T12:00:24.374142Z","iopub.status.idle":"2025-10-04T12:00:30.518726Z","shell.execute_reply.started":"2025-10-04T12:00:24.374127Z","shell.execute_reply":"2025-10-04T12:00:30.518000Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: albumentations==1.4.3 in /usr/local/lib/python3.11/dist-packages (1.4.3)\nRequirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.3) (1.26.4)\nRequirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.3) (1.15.3)\nRequirement already satisfied: scikit-image>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.3) (0.25.2)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.3) (6.0.2)\nRequirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.3) (4.14.0)\nRequirement already satisfied: scikit-learn>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.3) (1.7.2)\nRequirement already satisfied: opencv-python-headless>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.3) (4.11.0.86)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->albumentations==1.4.3) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->albumentations==1.4.3) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->albumentations==1.4.3) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->albumentations==1.4.3) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->albumentations==1.4.3) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->albumentations==1.4.3) (2.4.1)\nRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.3) (3.5)\nRequirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.3) (11.2.1)\nRequirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.3) (2.37.0)\nRequirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.3) (2025.6.11)\nRequirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.3) (25.0)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.3) (0.4)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.2->albumentations==1.4.3) (1.5.1)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.2->albumentations==1.4.3) (3.6.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24.4->albumentations==1.4.3) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24.4->albumentations==1.4.3) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.24.4->albumentations==1.4.3) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.24.4->albumentations==1.4.3) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.24.4->albumentations==1.4.3) (2024.2.0)\nRequirement already satisfied: torchsummary in /usr/local/lib/python3.11/dist-packages (1.5.1)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# CUDA?\ncuda = torch.cuda.is_available()\nprint(\"CUDA Available?\", cuda)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T12:00:30.519596Z","iopub.execute_input":"2025-10-04T12:00:30.519883Z","iopub.status.idle":"2025-10-04T12:00:30.524709Z","shell.execute_reply.started":"2025-10-04T12:00:30.519851Z","shell.execute_reply":"2025-10-04T12:00:30.524106Z"}},"outputs":[{"name":"stdout","text":"CUDA Available? True\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# ===============================\n# ðŸ“Œ Block 2: Define Albumentations for CIFAR-10\n# ===============================\n# CIFAR-10 mean & std\ncifar10_mean = (0.4914, 0.4822, 0.4465)\ncifar10_std = (0.2023, 0.1994, 0.2010)\n\ntrain_transforms = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n    A.CoarseDropout(max_holes=1, max_height=16, max_width=16,\n                    min_holes=1, min_height=16, min_width=16,\n                    fill_value=cifar10_mean, mask_fill_value=None, p=0.5),\n    A.Normalize(mean=cifar10_mean, std=cifar10_std),\n    ToTensorV2()\n])\n\ntest_transforms = A.Compose([\n    A.Normalize(mean=cifar10_mean, std=cifar10_std),\n    ToTensorV2()\n])\n\n# Custom Dataset wrapper for Albumentations\nfrom torchvision.datasets import CIFAR10\n\nclass CIFAR10_Augmented(CIFAR10):\n    def __init__(self, root, train=True, transform=None, download=False):\n        super().__init__(root=root, train=train, download=download)\n        self.transform = transform\n\n    def __getitem__(self, idx):\n        image, label = self.data[idx], self.targets[idx]\n        if self.transform:\n            image = self.transform(image=image)['image']\n        return image, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T12:00:30.526181Z","iopub.execute_input":"2025-10-04T12:00:30.526379Z","iopub.status.idle":"2025-10-04T12:00:30.548921Z","shell.execute_reply.started":"2025-10-04T12:00:30.526365Z","shell.execute_reply":"2025-10-04T12:00:30.548274Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# ===============================\n# ðŸ“Œ Block 3: Load Dataset & Dataloaders\n# ===============================\ntrain_dataset = CIFAR10_Augmented(root='./data', train=True, transform=train_transforms, download=True)\ntest_dataset = CIFAR10_Augmented(root='./data', train=False, transform=test_transforms, download=True)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n\nprint(f\"Train: {len(train_dataset)} | Test: {len(test_dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T12:00:30.549553Z","iopub.execute_input":"2025-10-04T12:00:30.549787Z","iopub.status.idle":"2025-10-04T12:00:32.160429Z","shell.execute_reply.started":"2025-10-04T12:00:30.549771Z","shell.execute_reply":"2025-10-04T12:00:32.159742Z"}},"outputs":[{"name":"stdout","text":"Train: 50000 | Test: 10000\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# ===============================\n# ðŸ“Œ Block 4: Define the Model\n# ===============================\nclass CIFARNet(nn.Module):\n    def __init__(self):\n        super(CIFARNet, self).__init__()\n\n        # C1 - Standard Conv i/p RF =1\n        #### BLOCK 1 #####\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1, bias=False)    #RF = 3\n        self.bn1 = nn.BatchNorm2d(16)\n        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, padding=1, bias= False)  ## RF = 5\n        self.bn2 = nn.BatchNorm2d(16)\n        #self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1, bias= False)  ## RF = 7\n        #self.bn3 = nn.BatchNorm2d(32)\n        self.depthwise1 = nn.Conv2d(16, 16, kernel_size=3, padding=1, groups=16, bias=False) # RF = 7\n        self.pointwise1 = nn.Conv2d(16, 32, kernel_size=1, bias=False)\n        self.bn4 = nn.BatchNorm2d(32)\n\n\n        self.dilatedconv1 = nn.Conv2d(32, 64, kernel_size=3, padding=1, dilation=2, bias=False) # RF = 11 ( EDGEs & Gradients)\n        self.bn5 = nn.BatchNorm2d(64)   #### Dilated Kernel\n        \n\n        self.antman1 = nn.Conv2d(64, 32, kernel_size=1, padding =0)\n\n\n\n\n        ### Block 2 ######\n        ### Textures & patterns ####\n\n        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1, bias= False)  ## RF = 13\n        self.bn6 = nn.BatchNorm2d(32)\n        \n\n\n\n        self.depthwise2 = nn.Conv2d(32, 32, kernel_size=3, padding=1, groups=32, bias=False) # RF = 17\n        self.pointwise2 = nn.Conv2d(32, 64, kernel_size=1, bias=False)\n        self.bn7 = nn.BatchNorm2d(64)\n        \n\n        #self.antman2 = nn.Conv2d(64, 32, kernel_size=1, padding =1)\n\n\n        #self.depthwise3 = nn.Conv2d(16, 16, kernel_size=3, padding=1, groups=32, bias=False)\n        #self.pointwise3 = nn.Conv2d(16, 32, kernel_size=1, bias=False)\n        #self.bn8 = nn.BatchNorm2d(32)\n\n        self.dilatedconv2 = nn.Conv2d(64, 64, kernel_size=3, padding=2, dilation=2, bias=False) # RF = 21\n        self.bn8 = nn.BatchNorm2d(64)\n\n        self.antman2 = nn.Conv2d(64, 32, kernel_size=1, padding =0)\n\n\n\n\n\n\n        #### Block 3 ######\n        #### Part of Objects ####\n\n        self.depthwise3 = nn.Conv2d(32, 32, kernel_size=3, padding=1, groups=32, bias=False) #RF == 23\n        self.pointwise3 = nn.Conv2d(32, 32, kernel_size=1, bias=False)\n        self.bn9 = nn.BatchNorm2d(32)\n        \n\n        self.dilatedconv3 = nn.Conv2d(32, 50, kernel_size=3, padding=2, dilation=2, bias=False) # RF = 27\n        self.bn10 = nn.BatchNorm2d(50)\n        #self.antman2 = nn.Conv2d(80, 64, kernel_size=1, padding =1) # RF = 13\n\n        self.conv5 = nn.Conv2d(50, 80, kernel_size=3, stride=2, padding=1, bias=False) ## RF= 29\n        self.bn11 = nn.BatchNorm2d(80)\n        self.dropout5 = nn.Dropout(0.05)\n\n        self.antman3 = nn.Conv2d(80, 32, kernel_size=1, padding =0)\n\n        self.depthwise4 = nn.Conv2d(32, 32, kernel_size=3, padding=1, groups=32, bias=False) #RF == 33\n        self.pointwise4 = nn.Conv2d(32, 32, kernel_size=1, bias=False)\n        self.bn12 = nn.BatchNorm2d(32)\n\n\n       # self.conv4 = nn.Conv2d(64, 80, kernel_size=3, padding=2, dilation=2, bias=False)\n       # self.bn8 = nn.BatchNorm2d(80)\n       # self.antman3 = nn.Conv2d(80, 64, kernel_size=1, padding =1)\n\n\n\n\n        ### Block 4 #######\n        ### Objects #######\n\n        self.conv6 = nn.Conv2d(32, 45, kernel_size=3,stride=2, padding=1, bias=False)    #RF = 35\n        self.bn13 = nn.BatchNorm2d(45)\n        self.dropout7 = nn.Dropout(0.05)\n\n        self.depthwise5 = nn.Conv2d(45, 45, kernel_size=3, padding=1, groups=45, bias=False) #RF == 39\n        self.pointwise5 = nn.Conv2d(45, 55, kernel_size=1, bias=False)\n        self.bn14 = nn.BatchNorm2d(55)\n        \n\n\n        self.dilatedconv4 = nn.Conv2d(55, 55, kernel_size=3, padding=2, dilation=2, bias=False) # RF = 43\n        self.bn15 = nn.BatchNorm2d(55)\n\n\n\n        self.conv7 = nn.Conv2d(55, 80, kernel_size=3, padding=1, bias=False) ## RF == 45\n\n       \n\n        # Global Average Pooling\n        self.gap = nn.AdaptiveAvgPool2d(1)\n\n        # Final Fully Connected Layer\n        self.fc = nn.Linear(80, 10, bias=False)\n\n    def forward(self, x):\n\n       ### BLOCK 1 ####\n        x = F.relu(self.bn1((self.conv1(x))))\n        x = F.relu(self.bn2((self.conv2(x))))\n        x = F.relu(self.bn4((self.pointwise1(self.depthwise1(x)))))\n        x = F.relu(self.bn5((self.dilatedconv1(x))))\n        x = self.antman1(x)\n\n        ### BLOCK2 ####\n\n        x = F.relu(self.bn6((self.conv3(x))))\n        x = F.relu(self.bn7((self.pointwise2(self.depthwise2(x)))))\n        x = F.relu(self.bn8((self.dilatedconv2(x))))\n        x = self.antman2(x)\n       # x = F.relu(self.bn6((self.pointwise2(self.depthwise2(x))) ))\n\n\n        ## BLOCK3 #####\n\n        x = F.relu(self.bn9((self.pointwise3(self.depthwise3(x)))))\n        x = F.relu(self.bn10((self.dilatedconv3(x))))\n        x = F.relu(self.bn11((self.conv5(x)))) # Removed the extra call to conv5 and directly applied relu and batchnorm\n        x = self.dropout5(x)\n        x = self.antman3(x)\n        x = F.relu(self.bn12((self.pointwise4(self.depthwise4(x)))))\n        #x = self.dropout5(x)\n\n\n        ### BLOCK 4 ####\n\n        x = F.relu(self.bn13((self.conv6(x))))\n        x = self.dropout7(x)\n        x = F.relu(self.bn14((self.pointwise5(self.depthwise5(x)))))\n        x = F.relu(self.bn15((self.dilatedconv4(x))))\n        x = self.conv7(x)\n\n\n\n        # Global Average Pooling\n        x = self.gap(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\nmodel = CIFARNet().cuda()\nsummary(model, input_size=(3, 32, 32))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T12:00:32.161209Z","iopub.execute_input":"2025-10-04T12:00:32.161486Z","iopub.status.idle":"2025-10-04T12:00:32.196604Z","shell.execute_reply.started":"2025-10-04T12:00:32.161461Z","shell.execute_reply":"2025-10-04T12:00:32.195903Z"}},"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 16, 32, 32]             432\n       BatchNorm2d-2           [-1, 16, 32, 32]              32\n            Conv2d-3           [-1, 16, 32, 32]           2,304\n       BatchNorm2d-4           [-1, 16, 32, 32]              32\n            Conv2d-5           [-1, 16, 32, 32]             144\n            Conv2d-6           [-1, 32, 32, 32]             512\n       BatchNorm2d-7           [-1, 32, 32, 32]              64\n            Conv2d-8           [-1, 64, 30, 30]          18,432\n       BatchNorm2d-9           [-1, 64, 30, 30]             128\n           Conv2d-10           [-1, 32, 30, 30]           2,080\n           Conv2d-11           [-1, 32, 15, 15]           9,216\n      BatchNorm2d-12           [-1, 32, 15, 15]              64\n           Conv2d-13           [-1, 32, 15, 15]             288\n           Conv2d-14           [-1, 64, 15, 15]           2,048\n      BatchNorm2d-15           [-1, 64, 15, 15]             128\n           Conv2d-16           [-1, 64, 15, 15]          36,864\n      BatchNorm2d-17           [-1, 64, 15, 15]             128\n           Conv2d-18           [-1, 32, 15, 15]           2,080\n           Conv2d-19           [-1, 32, 15, 15]             288\n           Conv2d-20           [-1, 32, 15, 15]           1,024\n      BatchNorm2d-21           [-1, 32, 15, 15]              64\n           Conv2d-22           [-1, 50, 15, 15]          14,400\n      BatchNorm2d-23           [-1, 50, 15, 15]             100\n           Conv2d-24             [-1, 80, 8, 8]          36,000\n      BatchNorm2d-25             [-1, 80, 8, 8]             160\n          Dropout-26             [-1, 80, 8, 8]               0\n           Conv2d-27             [-1, 32, 8, 8]           2,592\n           Conv2d-28             [-1, 32, 8, 8]             288\n           Conv2d-29             [-1, 32, 8, 8]           1,024\n      BatchNorm2d-30             [-1, 32, 8, 8]              64\n           Conv2d-31             [-1, 45, 4, 4]          12,960\n      BatchNorm2d-32             [-1, 45, 4, 4]              90\n          Dropout-33             [-1, 45, 4, 4]               0\n           Conv2d-34             [-1, 45, 4, 4]             405\n           Conv2d-35             [-1, 55, 4, 4]           2,475\n      BatchNorm2d-36             [-1, 55, 4, 4]             110\n           Conv2d-37             [-1, 55, 4, 4]          27,225\n      BatchNorm2d-38             [-1, 55, 4, 4]             110\n           Conv2d-39             [-1, 80, 4, 4]          39,600\nAdaptiveAvgPool2d-40             [-1, 80, 1, 1]               0\n           Linear-41                   [-1, 10]             800\n================================================================\nTotal params: 214,755\nTrainable params: 214,755\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.01\nForward/backward pass size (MB): 3.46\nParams size (MB): 0.82\nEstimated Total Size (MB): 4.29\n----------------------------------------------------------------\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# ===============================\n# ðŸ“Œ Block 5: Training & Testing Functions\n# ===============================\ndef train(model, device, train_loader, optimizer, criterion, epoch):\n    model.train()\n    total_loss, correct = 0, 0\n    for data, target in train_loader:\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        pred = output.argmax(dim=1)\n        correct += pred.eq(target).sum().item()\n\n    print(f\"Epoch {epoch} Train Loss: {total_loss/len(train_loader):.4f} | Acc: {100.*correct/len(train_loader.dataset):.2f}%\")\n\ndef test(model, device, test_loader, criterion):\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += criterion(output, target).item()\n            pred = output.argmax(dim=1)\n            correct += pred.eq(target).sum().item()\n\n    acc = 100.*correct/len(test_loader.dataset)\n    print(f\"Test Loss: {test_loss/len(test_loader):.4f} | Acc: {acc:.2f}%\")\n    return acc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T12:00:32.197393Z","iopub.execute_input":"2025-10-04T12:00:32.197588Z","iopub.status.idle":"2025-10-04T12:00:32.204542Z","shell.execute_reply.started":"2025-10-04T12:00:32.197573Z","shell.execute_reply":"2025-10-04T12:00:32.203869Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# ===============================\n# ðŸ“Œ Block 6: Run Training\n# ===============================\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\nbest_acc = 0\nfor epoch in range(1, 65):  # train for up to 65 epochs\n    train(model, device, train_loader, optimizer, criterion, epoch)\n    acc = test(model, device, test_loader, criterion)\n\n    if acc > best_acc:\n        best_acc = acc\n\nprint(\"Best Accuracy:\", best_acc)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T12:00:32.205361Z","iopub.execute_input":"2025-10-04T12:00:32.205621Z","iopub.status.idle":"2025-10-04T12:13:17.118026Z","shell.execute_reply.started":"2025-10-04T12:00:32.205606Z","shell.execute_reply":"2025-10-04T12:13:17.117065Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 Train Loss: 1.6722 | Acc: 36.67%\nTest Loss: 1.3992 | Acc: 49.04%\nEpoch 2 Train Loss: 1.3059 | Acc: 51.97%\nTest Loss: 1.2405 | Acc: 55.57%\nEpoch 3 Train Loss: 1.1597 | Acc: 58.00%\nTest Loss: 1.0348 | Acc: 63.67%\nEpoch 4 Train Loss: 1.0608 | Acc: 61.82%\nTest Loss: 0.8835 | Acc: 68.40%\nEpoch 5 Train Loss: 1.0021 | Acc: 64.35%\nTest Loss: 1.0171 | Acc: 64.32%\nEpoch 6 Train Loss: 0.9502 | Acc: 66.20%\nTest Loss: 0.8537 | Acc: 69.55%\nEpoch 7 Train Loss: 0.9050 | Acc: 68.09%\nTest Loss: 0.8057 | Acc: 71.89%\nEpoch 8 Train Loss: 0.8715 | Acc: 68.98%\nTest Loss: 0.7406 | Acc: 74.39%\nEpoch 9 Train Loss: 0.8414 | Acc: 70.08%\nTest Loss: 0.7047 | Acc: 75.21%\nEpoch 10 Train Loss: 0.8174 | Acc: 71.13%\nTest Loss: 0.6874 | Acc: 76.04%\nEpoch 11 Train Loss: 0.7960 | Acc: 71.90%\nTest Loss: 0.6614 | Acc: 77.10%\nEpoch 12 Train Loss: 0.7737 | Acc: 72.84%\nTest Loss: 0.7327 | Acc: 75.55%\nEpoch 13 Train Loss: 0.7627 | Acc: 73.31%\nTest Loss: 0.6088 | Acc: 79.15%\nEpoch 14 Train Loss: 0.7414 | Acc: 73.99%\nTest Loss: 0.5982 | Acc: 79.57%\nEpoch 15 Train Loss: 0.7235 | Acc: 74.71%\nTest Loss: 0.6694 | Acc: 78.21%\nEpoch 16 Train Loss: 0.7078 | Acc: 75.05%\nTest Loss: 0.6632 | Acc: 77.39%\nEpoch 17 Train Loss: 0.6965 | Acc: 75.48%\nTest Loss: 0.6424 | Acc: 78.60%\nEpoch 18 Train Loss: 0.6865 | Acc: 75.92%\nTest Loss: 0.5665 | Acc: 80.76%\nEpoch 19 Train Loss: 0.6787 | Acc: 76.18%\nTest Loss: 0.5939 | Acc: 80.13%\nEpoch 20 Train Loss: 0.6637 | Acc: 76.79%\nTest Loss: 0.6326 | Acc: 78.53%\nEpoch 21 Train Loss: 0.6516 | Acc: 77.14%\nTest Loss: 0.5512 | Acc: 81.32%\nEpoch 22 Train Loss: 0.6463 | Acc: 77.53%\nTest Loss: 0.5820 | Acc: 79.95%\nEpoch 23 Train Loss: 0.6360 | Acc: 77.86%\nTest Loss: 0.5254 | Acc: 81.96%\nEpoch 24 Train Loss: 0.6302 | Acc: 77.81%\nTest Loss: 0.4958 | Acc: 83.21%\nEpoch 25 Train Loss: 0.6208 | Acc: 78.56%\nTest Loss: 0.5102 | Acc: 82.40%\nEpoch 26 Train Loss: 0.6164 | Acc: 78.48%\nTest Loss: 0.5852 | Acc: 81.27%\nEpoch 27 Train Loss: 0.6035 | Acc: 78.79%\nTest Loss: 0.5463 | Acc: 81.62%\nEpoch 28 Train Loss: 0.5980 | Acc: 79.11%\nTest Loss: 0.5083 | Acc: 82.37%\nEpoch 29 Train Loss: 0.5879 | Acc: 79.43%\nTest Loss: 0.5086 | Acc: 82.52%\nEpoch 30 Train Loss: 0.5857 | Acc: 79.49%\nTest Loss: 0.6025 | Acc: 80.28%\nEpoch 31 Train Loss: 0.5847 | Acc: 79.47%\nTest Loss: 0.4958 | Acc: 83.14%\nEpoch 32 Train Loss: 0.5708 | Acc: 80.03%\nTest Loss: 0.5291 | Acc: 82.06%\nEpoch 33 Train Loss: 0.5662 | Acc: 80.23%\nTest Loss: 0.5473 | Acc: 81.97%\nEpoch 34 Train Loss: 0.5641 | Acc: 80.22%\nTest Loss: 0.4687 | Acc: 84.40%\nEpoch 35 Train Loss: 0.5558 | Acc: 80.63%\nTest Loss: 0.4726 | Acc: 84.28%\nEpoch 36 Train Loss: 0.5569 | Acc: 80.58%\nTest Loss: 0.4748 | Acc: 84.07%\nEpoch 37 Train Loss: 0.5452 | Acc: 80.93%\nTest Loss: 0.4586 | Acc: 84.46%\nEpoch 38 Train Loss: 0.5507 | Acc: 80.66%\nTest Loss: 0.4946 | Acc: 84.00%\nEpoch 39 Train Loss: 0.5436 | Acc: 80.98%\nTest Loss: 0.4789 | Acc: 84.26%\nEpoch 40 Train Loss: 0.5376 | Acc: 81.07%\nTest Loss: 0.4626 | Acc: 84.93%\nEpoch 41 Train Loss: 0.5315 | Acc: 81.57%\nTest Loss: 0.4737 | Acc: 84.25%\nEpoch 42 Train Loss: 0.5278 | Acc: 81.46%\nTest Loss: 0.4552 | Acc: 85.16%\nEpoch 43 Train Loss: 0.5221 | Acc: 81.81%\nTest Loss: 0.4668 | Acc: 84.44%\nEpoch 44 Train Loss: 0.5237 | Acc: 81.70%\nTest Loss: 0.4665 | Acc: 84.23%\nEpoch 45 Train Loss: 0.5162 | Acc: 81.93%\nTest Loss: 0.4553 | Acc: 84.50%\nEpoch 46 Train Loss: 0.5125 | Acc: 82.12%\nTest Loss: 0.4961 | Acc: 83.86%\nEpoch 47 Train Loss: 0.5049 | Acc: 82.38%\nTest Loss: 0.4418 | Acc: 85.02%\nEpoch 48 Train Loss: 0.5123 | Acc: 82.16%\nTest Loss: 0.4711 | Acc: 84.37%\nEpoch 49 Train Loss: 0.4996 | Acc: 82.50%\nTest Loss: 0.4274 | Acc: 85.93%\nEpoch 50 Train Loss: 0.5002 | Acc: 82.49%\nTest Loss: 0.4428 | Acc: 85.29%\nEpoch 51 Train Loss: 0.4995 | Acc: 82.63%\nTest Loss: 0.4425 | Acc: 85.30%\nEpoch 52 Train Loss: 0.4907 | Acc: 82.94%\nTest Loss: 0.4311 | Acc: 85.77%\nEpoch 53 Train Loss: 0.4903 | Acc: 82.77%\nTest Loss: 0.4211 | Acc: 86.12%\nEpoch 54 Train Loss: 0.4893 | Acc: 82.82%\nTest Loss: 0.4480 | Acc: 85.46%\nEpoch 55 Train Loss: 0.4811 | Acc: 83.13%\nTest Loss: 0.4411 | Acc: 85.46%\nEpoch 56 Train Loss: 0.4822 | Acc: 83.30%\nTest Loss: 0.4580 | Acc: 84.75%\nEpoch 57 Train Loss: 0.4804 | Acc: 83.26%\nTest Loss: 0.4349 | Acc: 85.66%\nEpoch 58 Train Loss: 0.4718 | Acc: 83.53%\nTest Loss: 0.4212 | Acc: 86.19%\nEpoch 59 Train Loss: 0.4795 | Acc: 83.45%\nTest Loss: 0.4187 | Acc: 85.91%\nEpoch 60 Train Loss: 0.4719 | Acc: 83.68%\nTest Loss: 0.4615 | Acc: 84.79%\nEpoch 61 Train Loss: 0.4746 | Acc: 83.41%\nTest Loss: 0.4072 | Acc: 86.57%\nEpoch 62 Train Loss: 0.4653 | Acc: 83.57%\nTest Loss: 0.4037 | Acc: 86.48%\nEpoch 63 Train Loss: 0.4690 | Acc: 83.62%\nTest Loss: 0.4321 | Acc: 85.52%\nEpoch 64 Train Loss: 0.4598 | Acc: 83.82%\nTest Loss: 0.3934 | Acc: 86.98%\nBest Accuracy: 86.98\n","output_type":"stream"}],"execution_count":31}]}